
import streamlit as st
import pandas as pd
from datetime import datetime
import os

st.set_page_config(page_title="SwarmAgent (Streamlit Demo)", layout="wide")

@st.cache_data
def load_data(tasks_path: str, cards_path: str):
    tasks = pd.read_csv(tasks_path)
    cards = pd.read_csv(cards_path)
    cards["cross_product"] = cards["a_prod"] != cards["b_prod"]
    cards["signals_count"] = cards["signals"].fillna("").apply(lambda s: len([x for x in s.split(", ") if x]))
    return tasks, cards

DATA_DIR = "data"
TASKS_CSV = os.path.join(DATA_DIR, "tasks.csv")
CARDS_CSV = os.path.join(DATA_DIR, "cards.csv")
# Choose a writable directory for actions log
ACTIONS_DIR = os.environ.get("ACTIONS_DIR", DATA_DIR)
try:
    # Ensure the directory exists & is writable, else fall back to /tmp
    os.makedirs(ACTIONS_DIR, exist_ok=True)
    test_path = os.path.join(ACTIONS_DIR, ".write_test")
    with open(test_path, "w") as _f:
        _f.write("ok")
    os.remove(test_path)
except Exception:
    ACTIONS_DIR = "/tmp"
ACTIONS_CSV = os.path.join(ACTIONS_DIR, "actions.csv")

tasks, cards = load_data(TASKS_CSV, CARDS_CSV)

st.sidebar.header("–§–∏–ª—å—Ç—Ä—ã")
type_filter = st.sidebar.multiselect("–¢–∏–ø –∫–∞—Ä—Ç–æ—á–∫–∏", options=sorted(cards["type"].unique()), default=list(sorted(cards["type"].unique())))
prod_filter = st.sidebar.multiselect("–ü—Ä–æ–¥—É–∫—Ç—ã", options=sorted(pd.unique(tasks["product"])), default=list(sorted(pd.unique(tasks["product"]))))
cap_filter = st.sidebar.multiselect("Capability", options=sorted(pd.unique(tasks["capability"])), default=[])
only_cross = st.sidebar.checkbox("–¢–æ–ª—å–∫–æ –∫—Ä–æ—Å—Å-–ø—Ä–æ–¥—É–∫—Ç", value=False)
min_score = st.sidebar.slider("–ú–∏–Ω. score (–¥–ª—è synergy/duplicate)", 0, 100, 50, step=5)
query = st.sidebar.text_input("–ü–æ–∏—Å–∫ –ø–æ —Ü–µ–ª–∏/–∑–∞–¥–∞—á–µ", "")

f_cards = cards[cards["type"].isin(type_filter)].copy()
f_cards = f_cards[(f_cards["a_prod"].isin(prod_filter)) | (f_cards["b_prod"].isin(prod_filter))]
if cap_filter:
    f_cards = f_cards[(f_cards["a_cap"].isin(cap_filter)) | (f_cards["b_cap"].isin(cap_filter))]
if only_cross:
    f_cards = f_cards[f_cards["cross_product"] == True]

def pass_score(row):
    if row["type"] in ["synergy", "duplicate"]:
        try:
            return int(row["score"]) >= min_score
        except Exception:
            return False
    return True
f_cards = f_cards[f_cards.apply(pass_score, axis=1)]

t = tasks.set_index("task_id")
def task_line(tid: str):
    if tid in t.index:
        row = t.loc[tid]
        return f"{tid} ¬∑ {row['product']}/{row['team']} ¬∑ {row['capability']} ¬∑ {row['goal'][:80]}"
    return tid

st.title("SwarmAgent ‚Äî Demo (–±–µ–∑ –≤—ã–∑–æ–≤–æ–≤ LLM)")
st.caption("–ü—Ä–µ–¥–∑–∞–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ capability/surface/entity/contract/KPI/goal.")

tab_inbox, tab_synergy, tab_conflicts, tab_duplicates, tab_overview = st.tabs(["üì¨ Inbox","ü§ù Synergy","‚ö†Ô∏è Conflicts","üîÅ Duplicates","üìä Overview"])

with tab_inbox:
    st.subheader("–°–≤–µ–∂–∏–µ –∫–∞—Ä—Ç–æ—á–∫–∏ (—Ñ–∏–ª—å—Ç—Ä—ã —Å–ª–µ–≤–∞)")
    st.write(f"–ù–∞–π–¥–µ–Ω–æ: **{len(f_cards)}**")
    for _, row in f_cards.sort_values(by=["type","score","signals_count"], ascending=[True, False, False]).head(100).iterrows():
        with st.expander(f"[{row['type'].upper()}] {task_line(row['a_id'])} ‚ü∑ {task_line(row['b_id'])} (signals: {row['signals']} | score: {row['score']} | cross: {'yes' if row['cross_product'] else 'no'})"):
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**A:** " + task_line(row['a_id']))
                st.json(t.loc[row['a_id']][["product","team","capability","surface","entity","contract","kpi_family","lever","goal","timeline_start","timeline_end"]].to_dict())
            with col2:
                st.markdown("**B:** " + task_line(row['b_id']))
                st.json(t.loc[row['b_id']][["product","team","capability","surface","entity","contract","kpi_family","lever","goal","timeline_start","timeline_end"]].to_dict())
            st.markdown("**Signals:** " + (row["signals"] or ""))
            st.markdown("---")
            st.markdown("**–î–µ–π—Å—Ç–≤–∏—è:**")
            act = st.radio("–í—ã–±–µ—Ä–∏ –¥–µ–π—Å—Ç–≤–∏–µ", ["meet (20 –º–∏–Ω)","adr (—á–µ—Ä–Ω–æ–≤–∏–∫)","dismiss"], key=f"act_{row['match_id']}", horizontal=True)
            if st.button("–í—ã–ø–æ–ª–Ω–∏—Ç—å", key=f"btn_{row['match_id']}"):
                action_row = {
                    "ts": datetime.utcnow().isoformat(),
                    "match_id": row["match_id"],
                    "type": row["type"],
                    "action": act.split(" ")[0],
                    "a_id": row["a_id"],
                    "b_id": row["b_id"]
                }
                try:
                    if os.path.exists(ACTIONS_CSV):
                        log = pd.read_csv(ACTIONS_CSV)
                        log = pd.concat([log, pd.DataFrame([action_row])], ignore_index=True)
                    else:
                        log = pd.DataFrame([action_row])
                    log.to_csv(ACTIONS_CSV, index=False)
                    st.success("–ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ.")
                except Exception as e:
                    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ: {e}")

with tab_synergy:
    st.subheader("–°–∏–Ω–µ—Ä–≥–∏–∏")
    g = f_cards[f_cards["type"]=="synergy"].copy()
    st.write(f"–í—Å–µ–≥–æ —Å–∏–Ω–µ—Ä–≥–∏–π (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤): **{len(g)}**")
    st.dataframe(g[["match_id","a_id","a_prod","a_cap","b_id","b_prod","b_cap","score","signals"]], use_container_width=True)

with tab_conflicts:
    st.subheader("–ö–æ–Ω—Ñ–ª–∏–∫—Ç—ã")
    g = f_cards[f_cards["type"]=="conflict"].copy()
    st.write(f"–í—Å–µ–≥–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤: **{len(g)}**")
    st.dataframe(g[["match_id","a_id","a_prod","a_cap","b_id","b_prod","b_cap","signals"]], use_container_width=True)

with tab_duplicates:
    st.subheader("–î—É–±–ª–∏–∫–∞—Ç—ã")
    g = f_cards[f_cards["type"]=="duplicate"].copy()
    st.write(f"–í—Å–µ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: **{len(g)}**")
    st.dataframe(g[["match_id","a_id","a_prod","a_cap","b_id","b_prod","b_cap","score","signals"]], use_container_width=True)

with tab_overview:
    st.subheader("–û–±–∑–æ—Ä –ø–æ—Ä—Ç—Ñ–µ–ª—è")
    left, right = st.columns(2)
    with left:
        st.markdown("**–ö–∞—Ä—Ç–æ—á–∫–∏ –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º (A-—Å—Ç–æ—Ä–æ–Ω–∞):**")
        counts = f_cards.groupby(["a_prod","type"]).size().unstack(fill_value=0)
        st.dataframe(counts)
    with right:
        st.markdown("**–ö–∞—Ä—Ç–æ—á–∫–∏ –ø–æ capability (—Ç–æ–ø-15):**")
        cap_counts = pd.concat([f_cards["a_cap"], f_cards["b_cap"]]).value_counts().head(15)
        st.dataframe(cap_counts.to_frame("count"))
    st.markdown("---")
    st.markdown("**Tasks:**")
    st.dataframe(tasks[["task_id","product","team","capability","surface","entity","contract","kpi_family","lever","goal","timeline_start","timeline_end"]], use_container_width=True)

st.sidebar.markdown("---")
st.sidebar.header("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
u_tasks = st.sidebar.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å tasks.csv", type=["csv"], key="u_tasks")
u_cards = st.sidebar.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å cards.csv", type=["csv"], key="u_cards")
if st.sidebar.button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ CSV"):
    if u_tasks is not None:
        pd.read_csv(u_tasks).to_csv(TASKS_CSV, index=False)
    if u_cards is not None:
        pd.read_csv(u_cards).to_csv(CARDS_CSV, index=False)
    st.success("–î–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω—ã. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—É.")
